[
  {
    "id": "propositional_logic_bidirectional_dilemma_1",
    "text": "If I watch my diet, I will maintain good health. If I consume excessive junk food, I will become more susceptible to illnesses. We know that either I am committed to watching my diet or I am not prone to illnesses, but it is uncertain which of these statements is true. It is possible that I am only dedicated to maintaining a healthy diet, or alternatively, I may not be prone to illnesses, or even both statements could be true.",
    "logic_type": "propositional_logic",
    "pattern": "bidirectional_dilemma",
    "logify_latency_sec": 13.918119668960571,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129783 tokens (1783 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Can we say at least one of the following must always be true? (a) i will stay healthy and (b) i don't eat too much junk food",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129783 tokens (1783 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) i won't stay healthy and (b) i eat too much junk food",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129783 tokens (1783 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) i will stay healthy and (b) i eat too much junk food",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129783 tokens (1783 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) i won't stay healthy and (b) i don't eat too much junk food",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129783 tokens (1783 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_commutation_1",
    "text": "I am not sure if Jack studies hard for his exams or if he gets top grades in school, but one of those two things must be true. It is possible that Jack only studies hard for his exams and does not get top grades, or it could be the case that he gets top grades in school without putting in much effort. Perhaps, both scenarios are true, and Jack both studies hard for exams and gets top grades.",
    "logic_type": "propositional_logic",
    "pattern": "commutation",
    "logify_latency_sec": 14.254642963409424,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129816 tokens (1816 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Can we say at least one of the following must always be true? (a) he gets top grades in school or (b) he studies hard for his exams",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129816 tokens (1816 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he doesn't get top grades in school and (b) he studies hard for his exams",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129816 tokens (1816 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he gets top grades in school and (b) he doesn't study hard for his exams",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129816 tokens (1816 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he doesn't get top grades in school and (b) he doesn't study hard for his exams",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129816 tokens (1816 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_constructive_dilemma_1",
    "text": "Harry had two options for his day off: going to the park or going to the beach. If he decided to go to the park, it meant that he would have a lovely picnic with his family. On the other hand, if he chose the beach, he would be able to enjoy a refreshing swim in the ocean. The only thing that was certain was that either Harry would go to the park or he would go to the beach. It was unclear which option he would ultimately choose, as it could be that he would go to the park alone, go to the beach alone, or even do both activities.",
    "logic_type": "propositional_logic",
    "pattern": "constructive_dilemma",
    "logify_latency_sec": 13.537478685379028,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129968 tokens (1968 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Can we say at least one of the following must always be true? (a) he will have a picnic with his family and (b) he will swim in the ocean",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129968 tokens (1968 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he won't have a picnic with his family and (b) he will swim in the ocean",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129968 tokens (1968 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he will have a picnic with his family and (b) he won't swim in the ocean",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129968 tokens (1968 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he won't have a picnic with his family and (b) he won't swim in the ocean",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129968 tokens (1968 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_destructive_dilemma_1",
    "text": "If John drives a car, then he will reach my destination on time. On the other hand, if he rides a bike, then he will save money on fuel. However, it is evident that he will not reach his destination on time or save money on fuel. It is unclear which of these scenarios is true. It could be that he won't reach his destination on time, or it could be that he won't save money on fuel, or it is possible that both situations are true.",
    "logic_type": "propositional_logic",
    "pattern": "destructive_dilemma",
    "logify_latency_sec": 43.79907155036926,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129759 tokens (1759 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Can we say at least one of the following must always be true? (a) he doesn't drive a car and (b) he doesn't ride a bike",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129759 tokens (1759 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he drives a car and (b) he rides a bike",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129759 tokens (1759 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he doesn't drive a car and (b) he rides a bike",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129759 tokens (1759 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Can we say at least one of the following must always be true? (a) he drives a car and (b) he doesn't ride a bike",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129759 tokens (1759 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_disjunctive_syllogism_1",
    "text": "Either she is not making a cake for the party, or he is writing a report for his boss, or both. We don't know for sure which one is true. However, it turns out that she is indeed making a cake for the party.",
    "logic_type": "propositional_logic",
    "pattern": "disjunctive_syllogism",
    "logify_latency_sec": 14.477851629257202,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129727 tokens (1727 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Does this mean that he is writing a report for his boss?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129727 tokens (1727 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Does this imply that he isn't writing a report for his boss?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129727 tokens (1727 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_hypothetical_syllogism_1",
    "text": "If Jim cleans his room, he will receive a reward. And if he receives a reward, he will use it to buy a new toy. So, Jim decided to tidy up his room, hoping to earn a reward. He diligently gathered his clothes, organized his toys, and dusted every surface. After a few hours of hard work, Jim's room was spotless.",
    "logic_type": "propositional_logic",
    "pattern": "hypothetical_syllogism",
    "logify_latency_sec": 14.509379863739014,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129835 tokens (1835 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "If jim cleaned his room , does this imply that he will buy a new toy?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129835 tokens (1835 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "If jim didn't clean his room, does this entail that he won't buy a new toy?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129835 tokens (1835 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "If jim cleaned his room , does this imply that he won't buy a new toy?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129835 tokens (1835 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "If jim didn't clean his room, does this imply that he will buy a new toy?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129835 tokens (1835 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_material_implication_1",
    "text": "Once upon a time, there was a guy named John who was very conscious about his weight. He knew that if he didn't eat healthy, he wouldn't be able to maintain his desired weight. John understood the condition that if he didn't eat healthy, he wouldn't be able to gain weight.",
    "logic_type": "propositional_logic",
    "pattern": "material_implication",
    "logify_latency_sec": 14.465417385101318,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129648 tokens (1648 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Based on context, can we say, at least one of the following must always be true? (a) he is eating healthy and (b) he will not gain weight",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129648 tokens (1648 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Based on context, can we say, at least one of the following must always be true? (a) he is not eating healthy and (b) he will gain weight",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129648 tokens (1648 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Based on context, can we say, at least one of the following must always be true? (a) he is not eating healthy and (b) he will not gain weight",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129648 tokens (1648 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Based on context, can we say, at least one of the following must always be true? (a) he is eating healthy and (b) he will gain weight",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129648 tokens (1648 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  },
  {
    "id": "propositional_logic_modus_tollens_1",
    "text": "Liam had finished his work early for the day, which meant that he would typically have ordered pizza for dinner. However, on this particular day, he decided against ordering pizza and opted for something else instead.",
    "logic_type": "propositional_logic",
    "pattern": "modus_tollens",
    "logify_latency_sec": 14.094882011413574,
    "logify_cached": false,
    "logify_error": "Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129763 tokens (1763 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
    "questions": [
      {
        "query": "Does this imply that liam didn't finish his work early?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "yes",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129763 tokens (1763 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      },
      {
        "query": "Does this entail that liam finished his work early?",
        "predicted_answer": "UNCERTAIN",
        "confidence": 0.0,
        "ground_truth": "no",
        "query_latency_total_sec": 0.0,
        "query_error": "Logify failed: Error in LLM conversion: Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 128000 tokens. However, you requested about 129763 tokens (1763 of text input, 128000 in the output). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}"
      }
    ]
  }
]